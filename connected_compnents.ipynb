{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyspark\n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import hash\n",
    "from pyspark.sql.functions import lit\n",
    "import os\n",
    "import networkx as nx\n",
    "# %pyspark --packages graphframes:graphframes:0.8.2-spark2.4-s_2.11 pyspark-shell\n",
    "os.environ['JAVA_HOME'] = '/shared/centos7/oracle_java/jdk1.8.0_181'\n",
    "\n",
    "\n",
    "N = 64 # Change this to change the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "#                             .config(\"spark.memory.offHeap.size\",\"100g\")\\\n",
    "#                             .config(\"spark.executor.memory\", \"100g\")\\\n",
    "#                             .config(\"spark.driver.memory\", \"100g\")\\\n",
    "#                             .appName(\"Reddit Community Detection\").getOrCreate()\n",
    "# sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(spark, N):\n",
    "    \"\"\"Reads the Reddit Hyperlinks Body dataset and filters out only relationships with positive sentiment\n",
    "    Input: spark, N: number of partitions\n",
    "    Output: df: Spark DataFrame\"\"\"\n",
    "    df = spark.read.csv(\"soc-redditHyperlinks-body.tsv\", sep=\"\\t\", header=True, inferSchema=True).repartition(N, \"SOURCE_SUBREDDIT\")\n",
    "    df = df.filter(\"LINK_SENTIMENT == 1\")\n",
    "    df = df.withColumn(\"weightage\", lit(1.0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertices(df):\n",
    "    \"\"\"Gets all the node ids and names and returns it as a Spark DataFrame\"\"\"\n",
    "    vertices = df.selectExpr(\"SOURCE_SUBREDDIT as id\", \"SOURCE_SUBREDDIT as name\").union(df.selectExpr(\"TARGET_SUBREDDIT as id\", \"TARGET_SUBREDDIT as name\")).distinct()\n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get edges from dataframe\n",
    "def get_edges(df):\n",
    "    \"\"\" function to get edges with weights > 1 from spark dataframe returns nodes\"\"\"\n",
    "    edges = df.selectExpr(\"SOURCE_SUBREDDIT as src\", \"TARGET_SUBREDDIT as dst\", \"weightage as weightage\")#.distinct()\n",
    "    edges = edges.groupBy(\"src\", \"dst\").agg(sum(\"weightage\").alias(\"weights\")).sort(\"weights\", ascending=False).filter(\"weights > 1\")\n",
    "    #edges.show(n=10)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph frame from nodes edges\n",
    "def create_graph(nodes, edges, N):\n",
    "    g = GraphFrame(nodes.repartition(N, hash(nodes[\"id\"]) % N), edges)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate connected components from graph frame\n",
    "def connected_components(sc, g, filename):\n",
    "    \"\"\" takes in spark graph frame, generates connected components\n",
    "        saves as pickle object\"\"\"\n",
    "    start = time.time()\n",
    "    sc.setCheckpointDir(\"./checkpoints\")\n",
    "    cc = g.connectedComponents()\n",
    "    end = time.time()\n",
    "    diff = end-start\n",
    "    #print(diff)\n",
    "    cc.rdd.saveAsPickleFile(filename)\n",
    "    cc.show()\n",
    "    return diff\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_to_dataframe(sc, spark, filename):\n",
    "    \"\"\" reads in pickle object and converts to spark dataframe\"\"\"\n",
    "    pickleRdd = sc.pickleFile(filename).collect()\n",
    "    return spark.createDataFrame(pickleRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __main__():\n",
    "    spark = SparkSession.builder.config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "                            .config(\"spark.memory.offHeap.size\",\"100g\")\\\n",
    "                            .config(\"spark.executor.memory\", \"100g\")\\\n",
    "                            .config(\"spark.driver.memory\", \"100g\")\\\n",
    "                            .appName(\"Reddit Community Detection\").getOrCreate()\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    df = read_csv(spark, N)\n",
    "    edges = get_edges(df)\n",
    "    vertices = get_vertices(df)\n",
    "    edges.rdd.saveAsPickleFile('all_edges.pkl')\n",
    "    print(\"Edges count = \", edges.count())\n",
    "    df_edges = edges.toPandas()\n",
    "    G = nx.from_pandas_edgelist(df_edges, source='src', target='dst', edge_attr=None, create_using=nx.DiGraph(), edge_key=None)\n",
    "    nx.write_gexf(G, \"reddit_links.gexf\")\n",
    "    \n",
    "    gf = create_graph(vertices, edges, N)\n",
    "    time_for_cc = connected_components(sc, gf, 'connected_components.pkl')\n",
    "    print(\"Connected Components took {} seconds to execute for N={}\".format(time_for_cc, N))\n",
    "    df = pickle_to_dataframe(sc, spark, 'connected_components.pkl')\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/17 14:00:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/17 14:00:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges count =  31571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+\n",
      "|                  id|                name|    component|\n",
      "+--------------------+--------------------+-------------+\n",
      "|            weirdlit|            weirdlit|            1|\n",
      "|    conservativemeta|    conservativemeta|            1|\n",
      "|       aliceinchains|       aliceinchains|1400159338500|\n",
      "|politicalpartypowers|politicalpartypowers|1056561954928|\n",
      "|       reviewalaptop|       reviewalaptop| 300647710861|\n",
      "|         robbereddit|         robbereddit| 506806141065|\n",
      "|        levantinewar|        levantinewar|            1|\n",
      "|         lilwa_dexel|         lilwa_dexel|            1|\n",
      "|        toastmasters|        toastmasters| 163208757409|\n",
      "|  onepiececirclejerk|  onepiececirclejerk|            1|\n",
      "|                mcnn|                mcnn|            1|\n",
      "|            rpbleach|            rpbleach|  85899346097|\n",
      "|     mhocworldpowers|     mhocworldpowers| 369367187541|\n",
      "|             luwatch|             luwatch|1537598292064|\n",
      "|  askredditafterdark|  askredditafterdark|            1|\n",
      "|              jaguar|              jaguar|  94489280595|\n",
      "|           boburnham|           boburnham| 575525617681|\n",
      "|          c710lorado|          c710lorado|  94489280533|\n",
      "|       fantasystrike|       fantasystrike|  25769803828|\n",
      "|     1000wordstories|     1000wordstories| 850403524608|\n",
      "+--------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Connected Components took 699.356157541275 seconds to execute for N=64\n",
      "+----------------+----------------+-------------+\n",
      "|              id|            name|    component|\n",
      "+----------------+----------------+-------------+\n",
      "|dragonballgaming|dragonballgaming| 833223655466|\n",
      "|        demetraa|        demetraa| 283467841572|\n",
      "|          fodust|          fodust|  77309411389|\n",
      "|   detroitcityfc|   detroitcityfc| 970662608923|\n",
      "|        tulplaza|        tulplaza|            1|\n",
      "|        ikemains|        ikemains| 283467841614|\n",
      "|    duelterminal|    duelterminal|1451698946094|\n",
      "|        learncss|        learncss|1589137899613|\n",
      "|  shadowofmordor|  shadowofmordor|            1|\n",
      "|             jhu|             jhu|1314259992664|\n",
      "|         etcshow|         etcshow|            1|\n",
      "|      succession|      succession|            1|\n",
      "|        indesign|        indesign|1176821039170|\n",
      "|   centralvalley|   centralvalley|1382979469332|\n",
      "|      javascript|      javascript|            1|\n",
      "|      kazakhstan|      kazakhstan| 146028888148|\n",
      "|  fireflythegame|  fireflythegame|1451698946106|\n",
      "|   rfelectronics|   rfelectronics|   8589934724|\n",
      "|     nightlypick|     nightlypick| 901943132249|\n",
      "|   warcraftmovie|   warcraftmovie|1451698946214|\n",
      "+----------------+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__main__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedar_spark_env",
   "language": "python",
   "name": "kedar_spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
