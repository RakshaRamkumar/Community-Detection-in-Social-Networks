{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5173ce02-35a9-4d17-bf4d-e247f6423f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyspark\n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import hash\n",
    "import os\n",
    "from pyspark.ml.clustering import PowerIterationClustering\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import collect_list\n",
    "import networkx as nx\n",
    "from connected_components import *\n",
    "N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64990ef1-0481-44fb-9e2a-aa151e6b5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_cc(cc):\n",
    "    \"\"\" function to get largest connected component id\"\"\"\n",
    "    # cc.sort(\"component\").groupby(\"component\").agg(F.collect_list(\"id\").alias(\"subreddits\")).show()\n",
    "    largest_component = cc.groupby(\"component\").agg(F.count(\"id\").alias(\"component_size\"))\\\n",
    "                        .orderBy(F.desc(\"component_size\")).first()\n",
    "    return largest_component.__getitem__('component')\n",
    "    \n",
    "def get_new_vertices(cc):\n",
    "    \"\"\" function to get list of subreddits (nodes) in largest connected component \"\"\"\n",
    "    largest_component = get_largest_cc(cc)  \n",
    "    new_vertices = cc.filter(\"component == {}\".format(largest_component)).drop(\"component\")\n",
    "    valid_vertex_ids = set(new_vertices.select('id').rdd.flatMap(lambda x: x).collect())\n",
    "    return new_vertices, valid_vertex_ids\n",
    "\n",
    "def get_new_edges(sc, spark, valid_vertex_ids):\n",
    "    edges = pickle_to_dataframe(sc, spark, 'all_edges.pkl')\n",
    "    new_edges = edges.filter(col('src').isin(valid_vertex_ids)).drop(\"weights\")\n",
    "    return new_edges\n",
    "\n",
    "def get_communities_lpa(new_graph):\n",
    "    start = time.time()\n",
    "    communities = new_graph.labelPropagation(maxIter=7)\n",
    "    end = time.time()\n",
    "    diff = end-start\n",
    "    print(communities.select(countDistinct(\"label\")).take(1))\n",
    "    return communities, diff\n",
    "\n",
    "def prepare_data_evaluation(valid_vertex_ids, new_edges, communities):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list(valid_vertex_ids))\n",
    "    G.add_edges_from(new_edges.collect())\n",
    "#     edges_without_weights = new_edges.drop(\"weights\")\n",
    "#     G.add_edges_from(edges_without_weights.collect())\n",
    "\n",
    "    # Create a dictionary mapping node IDs to their corresponding communities\n",
    "    node_communities = {}\n",
    "    communities = communities.groupBy(\"cluster\").agg(collect_list(\"id\").alias(\"nodes\"))\n",
    "    for row in communities.collect():\n",
    "        for node in row[\"nodes\"]:\n",
    "            node_communities[node] = row[\"cluster\"]\n",
    "\n",
    "    communities_list = communities.collect()\n",
    "    nodes_list = [frozenset(row[\"nodes\"]) for row in communities_list]\n",
    "\n",
    "    return G, nodes_list\n",
    "\n",
    "def evaluate(G, nodes_list):\n",
    "    modularity = nx.algorithms.community.modularity(G, nodes_list)\n",
    "    coverage, performance = nx.algorithms.community.quality.partition_quality(G, nodes_list)\n",
    "    nodes_list_con = [list(x) for x in nodes_list]\n",
    "    conductance = [nx.algorithms.cuts.conductance(G, cluster_i) for cluster_i in nodes_list_con]  \n",
    "    c_arr = np.array(conductance)\n",
    "    conductance_val =  np.min(c_arr)\n",
    "\n",
    "    return modularity, conductance_val, performance, coverage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f0ef71-fb72-4405-99c7-75fe52c05ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_weights(edges, threshold):\n",
    "    return edges.filter(\"weights > {}\".format(threshold))\n",
    "\n",
    "def get_new_edges_PIC(sc, spark, valid_vertex_ids, min_degree):\n",
    "    edges = pickle_to_dataframe(sc, spark, 'all_edges.pkl')\n",
    "    new_edges = filter_weights(edges, min_degree)\n",
    "    new_edges = new_edges.filter(col('src').isin(valid_vertex_ids))\n",
    "    return new_edges\n",
    "\n",
    "def map_string_to_ints(vertices):\n",
    "    #srcs = [row['src'] for row in edges.select('src').collect()]\n",
    "    #dsts = [row['dst'] for row in edges.select('dst').collect()]\n",
    "    #all_nodes = list(set(srcs + dsts))\n",
    "    all_nodes = [row['id'] for row in vertices.select('id').collect()]\n",
    "    node_to_index = {}\n",
    "    index_to_node = {}\n",
    "    i = 0\n",
    "    for node in all_nodes:\n",
    "        node_to_index.update({node: i})\n",
    "        index_to_node.update({i:node})\n",
    "        i += 1\n",
    "    return node_to_index, index_to_node\n",
    "\n",
    "#nodes, edges are dataframes\n",
    "def save_to_gephi(nodes, edges, filename):\n",
    "    \"\"\" function to save from graph frames to gephi\n",
    "        takes in nodes, edges as df, and filename as string\n",
    "        converts to network x graph, then writes to gexf\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(list(nodes))\n",
    "    G.add_edges_from(edges.collect())\n",
    "#     edges_without_weights = edges.drop(\"weights\")\n",
    "#     G.add_edges_from(edges_without_weights.collect())\n",
    "    nx.write_gexf(G, filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1988f9ab-4c0d-4de6-9d80-3b82c80bbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_graph(num_nodes):\n",
    "    G = nx.gaussian_random_partition_graph(num_nodes, 150, 50, 0.25, 0.1)\n",
    "    G_pd = nx.to_pandas_edgelist(G)\n",
    "    G_pd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7ce42a-ac7e-4cd5-96a9-62a4a3aef185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __main__():\n",
    "    spark = SparkSession.builder.config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "                            .config(\"spark.memory.offHeap.size\",\"100g\")\\\n",
    "                            .config(\"spark.executor.memory\", \"100g\")\\\n",
    "                            .config(\"spark.driver.memory\", \"100g\")\\\n",
    "                            .appName(\"Reddit Community Detection\").getOrCreate()\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    #cc = pickle_to_dataframe(sc, spark, 'connected_components.pkl')\n",
    "    #new_vertices, valid_vertex_ids = get_new_vertices(cc)\n",
    "    #new_edges = get_new_edges(sc, spark, valid_vertex_ids)\n",
    "    #new_edges.show()\n",
    "    cc = pickle_to_dataframe(sc, spark, 'connected_components.pkl')\n",
    "    new_vertices, valid_vertex_ids = get_new_vertices(cc)\n",
    "    #new_vertices.show()\n",
    "    new_edges = get_new_edges_PIC(sc, spark, valid_vertex_ids, 1)\n",
    "    #new_edges.show()\n",
    "    n_i, i_n = map_string_to_ints(new_vertices)\n",
    "    old_edges = new_edges.drop('weights')\n",
    "    new_edges = new_edges.rdd.map(lambda x: (n_i[x[0]], n_i[x[1]], x[2])).toDF(['src', 'dst', 'weights'])\n",
    "    #new_edges = new_edges.repartition(N, hash(new_edges[\"src\"]) % N)   \n",
    "    #N = [2, 4, 8, 16, 32]\n",
    "    #N = [8, 16, 32]\n",
    "    #for n in N:\n",
    "    new_edges = new_edges.repartition(N)\n",
    "    #new_edges.show()\n",
    "    pic = PowerIterationClustering(k=65, srcCol= 'src', dstCol= 'dst', weightCol=\"weights\")\n",
    "    max_iter = 75\n",
    "    pic.setMaxIter(max_iter)\n",
    "    start = time.time()\n",
    "    assignments = pic.assignClusters(new_edges)\n",
    "    end = time.time()\n",
    "    diff = end-start\n",
    "    #assignments.sort(assignments.id).show()\n",
    "    new_vertices = new_vertices.rdd.map(lambda x: (n_i[x[0]], x[1])).toDF(['src', 'name'])\n",
    "    #new_vertices.show()\n",
    "    result = new_vertices.join(assignments, new_vertices.src == assignments.id, \"inner\").drop('id')\n",
    "    #result.show()\n",
    "    #id_label = communities.select('id', 'label').rdd.map(lambda x: (x[0], {\"label\": x[1]})).collect()\n",
    "    #result.show()\n",
    "    nodes = result.rdd.map(lambda x: (x[1], {'label':x[2]})).collect()\n",
    "    #print(\"NODES\")\n",
    "    #print(nodes[:10])\n",
    "    #print(\"EDGES\")\n",
    "    #old_edges.show()\n",
    "    save_to_gephi(nodes, old_edges, 'PIC_communities.gexf')\n",
    "    print(\"For N: \", N, \", Iter: \", max_iter)\n",
    "    print(\"PIC ran for {} seconds.\".format(diff))\n",
    "    assignments = assignments.rdd.map(lambda x: (i_n[x[0]], x[1])).toDF(['id', 'cluster'])\n",
    "    #assignments.show()\n",
    "    #print(\"VALID_VERTEX_IDS\")\n",
    "    #print(valid_vertex_ids)\n",
    "    G, nodes_list = prepare_data_evaluation(valid_vertex_ids, old_edges, assignments)\n",
    "    modularity, conductance, performance, coverage = evaluate(G, nodes_list)\n",
    "    print(\"PIC Evaluation Metrics :\")\n",
    "    print(\"Modularity score = \", modularity)\n",
    "    print(\"Conductance = \", conductance)\n",
    "    print(\"Performance = \", performance)\n",
    "    print(\"Coverage = \", coverage)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69160c77-500e-4dab-94ab-26d7a00b573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/26 20:24:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For N:  4 , Iter:  75\n",
      "PIC ran for 167.84344053268433 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIC Evaluation Metrics :\n",
      "Modularity score =  0.4082763021073951\n",
      "Conductance =  0.056\n",
      "Performance =  0.89349694010456\n",
      "Coverage =  0.5684957355272096\n"
     ]
    }
   ],
   "source": [
    "__main__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee79c90-e3b9-4f1f-85ab-d53deaa5b721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedar_spark_env",
   "language": "python",
   "name": "kedar_spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
